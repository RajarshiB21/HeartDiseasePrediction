# -*- coding: utf-8 -*-
"""HeartDiseasePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YVjAIDHlwYDwWIu3bVaU7M1yhlQ2gP02
"""

#HeartDisease Prediction using Heart Data Features

"""**Heart Disease Prediction using independent medical features from Patients**

*Importing the Dependencies*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

"""**Importing the Dataset from Files Section into a pandas Dataframe**"""

heart_data=pd.read_csv('/content/heart.csv')

heart_data.shape
#Displaying the number of rows and columns in the dataset

heart_data.head()
#Displaying the first 5 rows

heart_data.describe()
#Displaying the dataset in a statistical manner for better understanding

heart_data.isnull().sum()
#Finding out if there are any null values and if any how many

"""**Plotting :** Now we find the correlation between the variables and plot a heat map"""

#Plotting the HeatMap
correlation = heart_data.corr()
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Reds')

heart_data['target'].value_counts()
#Counting the number of positive and negative values with respect to the fact that whether or not the patient has a heart disease

#Splitting the dependent and independent variables into an X and Y variable 
x=heart_data.drop(columns='target',axis=1)
y=heart_data['target']
#Storing the target column values in the independent variable Y 
#Storing the rest of the variables after dropping the target column in X which in turn becomes the dependent variabes

#Printing the number of rows and columns of each variable 
print(x.shape)
print(y.shape)

#Printing the X and Y Variable to ensure that the data stroed in them is correct
print(x)
print(y)

"""**Splitting the data into Training and Test data** : 
we will be using stratify as the dataset is not large enough and there is a possibility that the y_test variable might only have 0's and only 1's.
Stratify helps us to split the data into training and test data with respect to the dependent variable Y
"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=0)

x_train.head()

x_test.head()

#Without stratify there is a possibility of uneven distribution of values in the Y variable : y_train, y_test
print(x.shape,x_train.shape,x_test.shape)

"""**Training and Building the Machine Learning Model**: For this experiment we will be using Logistic Regression since the dataset available to us is relatively small."""

model=LogisticRegression()
#As you can see we are currently going with only the default parameter. This will further be enhanced using hyperparameter tuning

#Model training and checking the prediction accuracy on the training data
model.fit(x_train,y_train)
train_pred=model.predict(x_train)
#Comparing the accuracy of the model by comparing the prediction of the model to the predefined labels in the dataset this will be done later down the line
print(train_pred)

accuracy1=accuracy_score(train_pred,y_train)
print(accuracy1)
#The Model gives us more than 80 percent accuracy on the training data as well

#prediction on the basis of the test data
test_pred=model.predict(x_test)
print(test_pred)

accuracy=accuracy_score(test_pred,y_test)
print(accuracy)
#The Model gives us more than 80 percent accuracy on the test data

"""**Making a predictive system**: This system is designed to take an input from the user where the user gives the model dependent variables and the model makes a prediction based on those variables to deliver a result"""

input_data=(56,1,1,120,240,0,1,169,0,0,0,0,2)
#This is where we will be taking the input
input_data_as_numpy=np.asarray(input_data)#Transforming the input data into a array for ease of access
input_data_reshaped=input_data_as_numpy.reshape(1,-1)#reshaping the data
prediction=model.predict(input_data_reshaped)
print(prediction)

#Using a simple if and else systen to display the output
if(prediction[0]==1):
  print('Patient has a Heart Condition and needs further medical examination')

else:
  print('Patient does not have a Heart Condition')

"""**Saving and Loading the trained model**"""

import pickle
#Pickle library helps us easily save and load trained machine learning models

filename='trained_model.sav'
#Coming up with a file name for our saved model
pickle.dump(model,open(filename,'wb'))
#Opening a file by the name of trained_model and we mention that it will be writing binary or wb

#Loading the saved model
loaded_model=pickle.load(open('trained_model.sav','rb'))
#rb stands for read binary

